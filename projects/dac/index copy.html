<center><h1 style="font-size:36px; font-weight:bold;">DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs</h1></center>

<p style="text-align:center;">
The International Conference on Learning Representations (ICLR) 2021 [Spotlight]<br><br>
<nobr>Aayam Shrestha</nobr> &emsp;&emsp; <nobr> Stefan Lee </nobr> &emsp;&emsp; <nobr>Prasad Tadepalli</nobr> &emsp;&emsp; <nobr>Alan Fern</nobr><br><br>
<nobr>Oregon State University</nobr>
</p>

<center>
<img style="vertical-align:middle" src="dac_teaser.png" width="100%" height="inherit"/>
</center>

<br>

<hr>

<h3 style="margin-bottom:20px;">Abstract</h3>

<p style="text-align:left;">
We study an approach to offline reinforcement learning (RL) based on optimally solving finitely-represented MDPs derived from a static dataset of experience. This approach can be applied on top of any learned representation and has the potential to easily support multiple solution objectives as well as zero-shot adjustment to changing environments and goals. Our main contribution is to introduce the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions for offline RL. DAC-MDPs are a non-parametric model that can leverage deep representations and account for limited data by introducing costs for exploiting under-represented parts of the model. In theory, we show conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also investigate the empirical behavior in a number of environments, including those with image-based observations. Overall, the experiments demonstrate that the framework can work in practice and scale to large complex offline RL problems.
</p>

<hr>

<h3 style="margin-top:30px;"> 
<ul style="list-style-type:none; padding-left:0;">
  <li>Paper: [<a href="https://openreview.net/pdf?id=eMP1j9efXtX">PDF</a>]</li>
  <li>Code: [<a href="https://github.com/idigitopia/dacmdp">GitHub</a>]</li>
  <li>Poster: [<a href="https://iclr.cc/virtual/2021/spotlight/3450">ICLR</a>]</li>  
  <li>Preprint: [<a href="https://arxiv.org/abs/2010.08891">arXiv</a>]</li>
</ul>
</h3>

<h3 style="margin-bottom:10px;">Videos:</h3>

<iframe width="560" height="315" src="https://www.youtube.com/embed/gecEH4iWMAU?si=xnYz1VNEghElcPM-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<br><br>

<h3 style="margin-bottom:10px;">Bibtex</h3>

<pre>
@article{
Shrestha2020DeepAveragersOR,
title={DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs},   
author={Aayam Shrestha and Stefan Lee and Prasad Tadepalli and Alan Fern},
journal={ICLR},
year={2021},
numpages = {24},
url = {https://iclr.cc/virtual/2021/poster/3092}
keywords = {non-parametric markov decision process, exact planning, offline reinforcement learning}  
}
</pre>